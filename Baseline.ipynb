{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Github__1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jaGXdX8TZjeW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "543wNsXnVjLC"
      },
      "source": [
        "<img src=\"https://aiisc.ai/defactify/img/factify_logo.png\" width=300 align=\"left\">\n",
        "<center>\n",
        "<h4>Baseline</h4>\n",
        "<p>Factify is a Multi-Modal Fact Verification dataset released for a shared task as part of the <a href=\"https://aiisc.ai/defactify/\">De-Factify workshop</a> in AAAI-21.</p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOStQu9DUkVn"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaGXdX8TZjeW"
      },
      "source": [
        "# Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW01nx61ZppW"
      },
      "source": [
        "Download the training data from the codalab competition page from the Participate > Files > Public Data\n",
        "or use the drive library below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJv3P3PT-4-X"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1ig7XEYU1UKDHrHgDYgqiARWvNdswgFEX',\n",
        "                                    dest_path='/content/public_folder.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3wo_U4q4Ccd"
      },
      "source": [
        "#fill password\n",
        "pswd=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqDwOJ4sYecJ"
      },
      "source": [
        "with zipfile.ZipFile(\"public_folder.zip\") as file:\n",
        "  file.extractall(pwd = bytes(pswd, 'utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlJkk04rApAJ"
      },
      "source": [
        "download_path = \"public_folder/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ydINvCaYEq"
      },
      "source": [
        "train_df = pd.read_csv(download_path + \"train.csv\", index_col=\"Id\")\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK5sCxImGIuw"
      },
      "source": [
        "train_df=train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBxPcX9JeyM5"
      },
      "source": [
        "You can save images as you like, but we provide a template to store claim and support images seperately in directories created class-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGGBOyGjbdWw"
      },
      "source": [
        "image_directory = \"images\"\n",
        "if not os.path.isdir(image_directory):\n",
        "  os.makedirs(image_directory)\n",
        "for i in [\"claim\", \"document\"]:\n",
        "  if not os.path.isdir(image_directory + \"/\" + i):\n",
        "    os.makedirs(image_directory + \"/\" + i)\n",
        "  for cls in [\"Support_Multimodal\", \"Support_Text\", \"Insufficient_Multimodal\", \"Insufficient_Text\", \"Refute\"]:\n",
        "    if not os.path.isdir(image_directory + \"/\" + i + \"/\" + cls):\n",
        "      os.makedirs(image_directory + \"/\" + i + \"/\" + cls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGtQn4TFFol5"
      },
      "source": [
        "for n, row in train_df.iterrows():\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "  response = requests.get(row[\"claim_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory + \"/claim/\" + row[\"Category\"] + \"/\" + str(n) + \".jpg\")\n",
        "\n",
        "  response = requests.get(row[\"document_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory + \"/document/\" + row[\"Category\"] + \"/\" + str(n) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvGbRIk5adRW"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OATJ1zkcah9j"
      },
      "source": [
        "# Model definition and training\n",
        "!pip install sentence-transformers\n",
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXfkJGJo0ypb"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import cv2\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from resnet50 import ResNet50\n",
        "import tensorflow\n",
        "from keras.layers import Input\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLzwQxEJ093z"
      },
      "source": [
        "def get_feature_vector_fromPIL(img):\n",
        "    feature_vector = feature_model.predict(img)\n",
        "    a, b, c, n = feature_vector.shape\n",
        "    feature_vector= feature_vector.reshape(b,n)\n",
        "    return feature_vector\n",
        "\n",
        "def calculate_similarity_cosine(vector1, vector2):\n",
        "    #return 1 - distance.cosine(vector1, vector2)\n",
        "    return cosine_similarity(vector1, vector2)\n",
        "\n",
        "# This distance can be in range of [0,âˆž]. And this distance is converted to a [0,1]\n",
        "def calculate_similarity_euclidean(vector1, vector2):\n",
        "    return 1/(1 + np.linalg.norm(vector1- vector2))  \n",
        "\n",
        "#Use ResNet-50 model as an image feature extractor\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "feature_model = ResNet50(input_tensor=image_input, include_top=False,weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgW88O0a1CKT"
      },
      "source": [
        "def cosinesimilarity_Resnet50(a1,a2):\n",
        "    x = image.load_img(a1, target_size=(224, 224))\n",
        "    x = image.img_to_array(x)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    x2 = image.load_img(a2, target_size=(224, 224))\n",
        "    x2 = image.img_to_array(x2)\n",
        "    x2 = np.expand_dims(x2, axis=0)\n",
        "    x2 = preprocess_input(x2)\n",
        "    image_similarity_cosine = calculate_similarity_cosine(get_feature_vector_fromPIL(x), get_feature_vector_fromPIL(x2))[0][0]\n",
        "    return image_similarity_cosine"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To2Tp9mt1ERa"
      },
      "source": [
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zHpZ8Uz1Mzb"
      },
      "source": [
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "em1 = sbert_model.encode(list(train_df['claim']))\n",
        "em2 = sbert_model.encode(list(train_df['document']))\n",
        "ps = PorterStemmer()\n",
        "sw = list(stopwords.words('english'))\n",
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6oul1Ac1PNt"
      },
      "source": [
        "text_sim = []\n",
        "img_sim = []\n",
        "for i in tqdm(range(len(em1))):\n",
        "    sim = cosine(em1[i], em2[i])\n",
        "    text_sim.append(sim)\n",
        "    \n",
        "    sim2 = cosinesimilarity_Resnet50(\n",
        "        'images/claim/' + train_df[\"Category\"].values[i] + \"/\" + str(train_df.index.values[i]) + \".jpg\",\n",
        "        'images/document/' + train_df[\"Category\"].values[i] + \"/\" + str(train_df.index.values[i]) + \".jpg\",\n",
        "        )\n",
        "    img_sim.append(sim2)\n",
        "\n",
        "train_df['Text_Sim'] = text_sim\n",
        "train_df['Img_Sim'] = img_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6rotcx__xkj"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfA-qmFNanAP"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEZsMGZugeWM"
      },
      "source": [
        "val_df = pd.read_csv(download_path + \"val.csv\", index_col=\"Id\")\n",
        "val_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPgM7MJtIWt9"
      },
      "source": [
        "image_directory_val = \"images_val\"\n",
        "if not os.path.isdir(image_directory_val):\n",
        "  os.makedirs(image_directory_val)\n",
        "for i in [\"claim\", \"document\"]:\n",
        "  if not os.path.isdir(image_directory_val + \"/\" + i):\n",
        "    os.makedirs(image_directory_val + \"/\" + i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfS5FL0GIZ1_"
      },
      "source": [
        "for n, row in val_df.iterrows():\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "  response = requests.get(row[\"claim_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory_val + \"/claim/\" + str(n) + \".jpg\")\n",
        "\n",
        "  response = requests.get(row[\"document_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory_val + \"/document/\" + str(n) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7XZJPSz2g7v"
      },
      "source": [
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "em1 = sbert_model.encode(list(val_df['claim']))\n",
        "em2 = sbert_model.encode(list(val_df['document']))\n",
        "ps = PorterStemmer()\n",
        "sw = list(stopwords.words('english'))\n",
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XviJFCpgt4g"
      },
      "source": [
        "# Validation data prediction using trained model\n",
        "text_sim = []\n",
        "img_sim = []\n",
        "for i in tqdm(range(len(em1))):\n",
        "    sim = cosine(em1[i], em2[i])\n",
        "    text_sim.append(sim)\n",
        "    \n",
        "    sim2 = cosinesimilarity_Resnet50(\n",
        "        'images_val/claim/' + str(val_df.index.values[i]) + \".jpg\",\n",
        "        'images_val/document/' + str(val_df.index.values[i]) + \".jpg\",\n",
        "        )\n",
        "    img_sim.append(sim2)\n",
        "\n",
        "val_df['Text_Sim'] = text_sim\n",
        "val_df['Img_Sim'] = img_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsD_KxuQCAly"
      },
      "source": [
        "## Text-only Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qkk9_7sAB_24"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5zKFKDWCHLr"
      },
      "source": [
        "X_train = train_df[[\"Text_Sim\"]]\n",
        "y_train = train_df[\"Category\"]\n",
        "\n",
        "X_test = val_df[[\"Text_Sim\"]]\n",
        "y_test = val_df[\"Category\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoE7tFDfCHRn"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train, y_train)\n",
        "predictions = neigh.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpq_ck6ACHVg"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvAg6WppCHZg"
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXJKjgWJCO9u"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69tc6yRECPBw"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=500,max_depth=10, random_state=16)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3uhJWG4gi8"
      },
      "source": [
        "## Multimodal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlRSp4KX3QR8"
      },
      "source": [
        "X_train = train_df[[\"Text_Sim\", \"Img_Sim\"]]\n",
        "y_train = train_df[\"Category\"]\n",
        "\n",
        "X_test = val_df[[\"Text_Sim\", \"Img_Sim\"]]\n",
        "y_test = val_df[\"Category\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQI7j6cqBEqp"
      },
      "source": [
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_train, y_train)\n",
        "predictions = neigh.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2ylGHXbBEyb"
      },
      "source": [
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='rbf'))\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImjIuTa2BE3u"
      },
      "source": [
        "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VP4MJHVBFAR"
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf = clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j56wORm-29yQ"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=500,max_depth=10, random_state=16)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "score = f1_score(y_test, predictions, average='weighted')\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI7ILczaB6WA"
      },
      "source": [
        "## Test "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJHMhMqpCyKq"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1dAlXZVDc09gmaEwPHHw8H2BCBYtDH4gz',\n",
        "                                    dest_path='/content/test.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_jm8DWDMMU"
      },
      "source": [
        "#fill password\n",
        "pswd=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6wvzO5LDN0w"
      },
      "source": [
        "with zipfile.ZipFile(\"test.zip\") as file:\n",
        "  file.extractall(pwd = bytes(pswd, 'utf-8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJkl2W_DN44"
      },
      "source": [
        "download_path = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2UGfr5DSMh"
      },
      "source": [
        "test_df = pd.read_csv(download_path + \"test.csv\", index_col=\"Id\")\n",
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6-tqTJZDSQ9"
      },
      "source": [
        "image_directory_test = \"images_test\"\n",
        "if not os.path.isdir(image_directory_test):\n",
        "  os.makedirs(image_directory_test)\n",
        "for i in [\"claim\", \"document\"]:\n",
        "  if not os.path.isdir(image_directory_test + \"/\" + i):\n",
        "    os.makedirs(image_directory_test + \"/\" + i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT3EqVrLDHr0"
      },
      "source": [
        "for n, row in test_df.iterrows():\n",
        "  headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
        "  response = requests.get(row[\"claim_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory_test + \"/claim/\" + str(n) + \".jpg\")\n",
        "\n",
        "  response = requests.get(row[\"document_image\"], headers=headers)\n",
        "  img = Image.open(io.BytesIO(response.content))\n",
        "  img = img.convert('RGB')\n",
        "  img.save(image_directory_test + \"/document/\" + str(n) + \".jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa3570hHDxWY"
      },
      "source": [
        "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "em1 = sbert_model.encode(list(test_df['claim']))\n",
        "em2 = sbert_model.encode(list(test_df['document']))\n",
        "ps = PorterStemmer()\n",
        "sw = list(stopwords.words('english'))\n",
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgvCf79zDxbN"
      },
      "source": [
        "# Test data prediction using trained model\n",
        "text_sim = []\n",
        "img_sim = []\n",
        "for i in tqdm(range(len(em1))):\n",
        "    sim = cosine(em1[i], em2[i])\n",
        "    text_sim.append(sim)\n",
        "    \n",
        "    sim2 = cosinesimilarity_Resnet50(\n",
        "        'images_test/claim/' + str(test_df.index.values[i]) + \".jpg\",\n",
        "        'images_test/document/' + str(test_df.index.values[i]) + \".jpg\",\n",
        "        )\n",
        "    img_sim.append(sim2)\n",
        "\n",
        "test_df['Text_Sim'] = text_sim\n",
        "test_df['Img_Sim'] = img_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb1u_dlnEEkB"
      },
      "source": [
        "# Multimodal Model\n",
        "\n",
        "X_train = train_df[[\"Text_Sim\", \"Img_Sim\"]]\n",
        "y_train = train_df[\"Category\"]\n",
        "\n",
        "X_test = test_df[[\"Text_Sim\", \"Img_Sim\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBbxzUKMEEos"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=500,max_depth=10, random_state=16)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4LqDffgaug3"
      },
      "source": [
        "res = {\"Id\": test_df.index, \"Category\": predictions}\n",
        "answer = pd.DataFrame(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Djo3kg-bHAZ"
      },
      "source": [
        "answer.to_csv(\"answer.csv\",index=False)\n",
        "!zip answer.zip answer.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqV_aOQgF2C8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}